{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv8 Architecture: Built from Scratch\n",
    "\n",
    "This notebook presents a simple implementation of the **YOLOv8 (You Only Look Once, Version 8)** object detection architecture from scratch. YOLOv8 is a state-of-the-art, real-time object detection model that emphasizes speed and accuracy, using an anchor-free approach and improved backbone and head designs from its previous versions.\n",
    "\n",
    "---\n",
    "\n",
    "###  Contributors:\n",
    "\n",
    "- **Pranay Gadh** – Delhi Technological University (DTU)  \n",
    "- **Divansu Mishra** – Delhi Technological University (DTU)  \n",
    "- **Aarushi Anand** – Delhi Technological University (DTU)  \n",
    "\n",
    "---\n",
    "\n",
    "This implementation aims to provide educational insights into the core building blocks of YOLOv8 while allowing for customization and experimentation in research and applied settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "47e5f16c-b509-4b5a-9e98-a68dd525bd3a",
    "_kg_hide-output": false,
    "_uuid": "24f7dda4-6ce0-4f5d-9271-c9ace925c7eb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:25.323487Z",
     "iopub.status.busy": "2025-06-16T11:42:25.322742Z",
     "iopub.status.idle": "2025-06-16T11:42:27.721888Z",
     "shell.execute_reply": "2025-06-16T11:42:27.721147Z",
     "shell.execute_reply.started": "2025-06-16T11:42:25.323458Z"
    },
    "id": "d9a4d0Rqw8AT",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "f15737ac-ce0b-47e6-dcaf-b82df199166c",
    "trusted": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.13' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "cc5f7a99-9d5d-4349-80a4-4c1bfe69d73a",
    "_uuid": "1a6f48c1-5120-42fb-a688-da89fa4c46e3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:27.723613Z",
     "iopub.status.busy": "2025-06-16T11:42:27.723275Z",
     "iopub.status.idle": "2025-06-16T11:42:37.305866Z",
     "shell.execute_reply": "2025-06-16T11:42:37.305352Z",
     "shell.execute_reply.started": "2025-06-16T11:42:27.723594Z"
    },
    "id": "T1cW7zjPw8AT",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f9a313a1-5696-4d23-8e91-fcd3948475a7",
    "_uuid": "2a843fec-2cc9-42cb-b053-fe63f46378ad",
    "collapsed": false,
    "id": "HE-0IXJdw8AT",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Step 1: ConvBNAct Block\n",
    "Converts our input image (B, 3,640,640) into (B,64, 320,320)![image.png](images-ref/aba6bf98-d3f5-483b-bc32-61141f4981c9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "a0e73367-1604-4a09-9855-bdd812388e92",
    "_uuid": "c72aab88-6eaa-4990-b061-86e5e063cbf4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:37.307012Z",
     "iopub.status.busy": "2025-06-16T11:42:37.306640Z",
     "iopub.status.idle": "2025-06-16T11:42:37.311635Z",
     "shell.execute_reply": "2025-06-16T11:42:37.311096Z",
     "shell.execute_reply.started": "2025-06-16T11:42:37.306988Z"
    },
    "id": "6n9aHCu1w8AU",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ConvBNAct (nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride,padding, bias = False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.SiLU())\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "2d373304-1675-462c-8cb9-2dc51651133d",
    "_uuid": "4b9cc167-688e-42ad-aec8-f370a2cee95c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:37.313416Z",
     "iopub.status.busy": "2025-06-16T11:42:37.313154Z",
     "iopub.status.idle": "2025-06-16T11:42:42.231862Z",
     "shell.execute_reply": "2025-06-16T11:42:42.231097Z",
     "shell.execute_reply.started": "2025-06-16T11:42:37.313398Z"
    },
    "id": "N44M70z6w8AU",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#image_test\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "image = Image.open(\"/kaggle/input/traffic-data/traffic_wala_dataset/train/images/10_mp4-10_jpg.rf.b87509668caff369c5501325477e6d9a.jpg\").convert(\"RGB\")\n",
    "transf = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "image_tensor = transf(image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "6bf65fec-30dc-4e4c-a8e1-8a4842d47291",
    "_uuid": "c9b956c7-3e57-4030-b915-dd89a7d8aeb4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:42.233028Z",
     "iopub.status.busy": "2025-06-16T11:42:42.232717Z",
     "iopub.status.idle": "2025-06-16T11:42:42.519796Z",
     "shell.execute_reply": "2025-06-16T11:42:42.518891Z",
     "shell.execute_reply.started": "2025-06-16T11:42:42.233011Z"
    },
    "id": "IvWm2ueLw8AU",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "e07f13da-43bb-4b9d-a95c-693b6e32773f",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 320, 320])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvBNAct(in_channels = 3, out_channels = 64, kernel_size = 3, stride = 2, padding = 1)\n",
    "output = model(image_tensor)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "aa6729fa-2c6e-4f54-ae90-d9b4491ceb77",
    "_uuid": "dc1c6b62-acc6-4f62-8f3c-f854d013f5a9",
    "collapsed": false,
    "id": "rC96aMZWw8AU",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Step2: BottleNeck Block:\n",
    "Extracts importat features from our image\n",
    "<br>\n",
    "Input:  (B, 64, 320, 320) -->\n",
    "        1×1 conv → (B, 32, 320, 320)-->\n",
    "        3×3 conv → (B, 64, 320, 320)-->\n",
    "<br>\n",
    "Residual: if allowed --> out = out + input\n",
    "![image.png](images-ref/4fc95751-15e4-4c2c-a92c-6a2cd558b78e.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "ac586fad-6e5f-4383-9bc4-101783979e4d",
    "_uuid": "af6b318a-fdf7-4342-a71b-f62431cb9ffe",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:42.521137Z",
     "iopub.status.busy": "2025-06-16T11:42:42.520817Z",
     "iopub.status.idle": "2025-06-16T11:42:42.527174Z",
     "shell.execute_reply": "2025-06-16T11:42:42.526484Z",
     "shell.execute_reply.started": "2025-06-16T11:42:42.521109Z"
    },
    "id": "yQEkEPe4w8AU",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    def __init__ (self, in_channels, out_channels, shortcut = True, expansion = 0.5):\n",
    "        super().__init__()\n",
    "        hidden_channels = int(out_channels * expansion)\n",
    "        self.conv1 = ConvBNAct(in_channels, hidden_channels, stride = 1, padding = 0,kernel_size = 1)\n",
    "        self.conv2 = ConvBNAct(hidden_channels, out_channels, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.residual = shortcut and (in_channels == out_channels)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.conv2(self.conv1(x))\n",
    "        if (self.residual == True):\n",
    "            out = out + x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "e21688d6-9b27-417a-9f76-b4c77f303fd6",
    "_uuid": "c611c3f9-5c04-4c49-ac12-7cfd58c6df9a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:42.528032Z",
     "iopub.status.busy": "2025-06-16T11:42:42.527856Z",
     "iopub.status.idle": "2025-06-16T11:42:42.675734Z",
     "shell.execute_reply": "2025-06-16T11:42:42.675031Z",
     "shell.execute_reply.started": "2025-06-16T11:42:42.528017Z"
    },
    "id": "0oR-aARMw8AU",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "2fc18831-3620-4fbd-ceb9-3bc6f9e447df",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 320, 320])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Bottleneck(in_channels = 64, out_channels = 64)\n",
    "output2 = model2(output)\n",
    "output2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b4c24943-0640-4249-b73c-7be31e75c72d",
    "_uuid": "5c4a2fcf-d79d-4edc-8a40-043e1b4b3098",
    "collapsed": false,
    "id": "icRiSmTFw8AU",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## C2f Block:\n",
    "C2f = \"Concatenate → Convolution → f-block (like Bottleneck)\"\n",
    "![image.png](images-ref/3f9e9a8c-85a1-4718-88a7-526bed929a51.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "2c8c5776-f5db-4a8a-89d3-7b66232dd614",
    "_uuid": "4b55da4c-f8eb-45a6-a1f5-56da30e80fff",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:42.676743Z",
     "iopub.status.busy": "2025-06-16T11:42:42.676511Z",
     "iopub.status.idle": "2025-06-16T11:42:42.683939Z",
     "shell.execute_reply": "2025-06-16T11:42:42.683285Z",
     "shell.execute_reply.started": "2025-06-16T11:42:42.676725Z"
    },
    "id": "bvXgCrH8w8AU",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class c2f(nn.Module):\n",
    "    def __init__ (self, in_channels, out_channels, n, d,expansion = 0.5, shortcut = True):\n",
    "        super().__init__()\n",
    "        hidden_channels = int(out_channels * expansion)\n",
    "        num_bnecks = max(round(n * d), 1)\n",
    "        #1. applying 1st conv layer from ConvBNAct\n",
    "        self.conv1 = ConvBNAct(in_channels, out_channels, kernel_size = 1, stride = 1, padding = 0)\n",
    "        #2. splitting this output in 2 parts:\n",
    "        #   one for passing into bottlenecks and other as it is (for skip connections behaviour)\n",
    "        self.layers = nn.ModuleList([\n",
    "            Bottleneck(hidden_channels, hidden_channels) for i in range(num_bnecks)\n",
    "        ])\n",
    "        #3. applying second conv layer\n",
    "        self.conv2 = ConvBNAct((num_bnecks + 2)*hidden_channels, out_channels, kernel_size = 1, stride = 1, padding = 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #1. Input Image -->CONV1 LAYER --> increased channels wali image\n",
    "        x = self.conv1(x)\n",
    "        #2. splitting this obtained image into 2 parts channel-wise\n",
    "        x1 = x[:, :x.shape[1]//2, :, :]   # shape: B, 64, 320, 320\n",
    "        x2 = x[:, x.shape[1]//2:, :, :]   # shape: B, 64, 320, 320\n",
    "\n",
    "        outputs = [x1,x2]\n",
    "        # now we let remain x1 as it is, and pass x2 into bottle necks\n",
    "        for layer in self.layers:\n",
    "            x1 = layer(x1)\n",
    "            outputs.append(x1)\n",
    "\n",
    "        out_final = torch.concat(outputs, dim = 1) # dim = 1 for concatinating along channels\n",
    "\n",
    "        #3. now, we pass this concatinated output into conv2 layer\n",
    "        out = self.conv2(out_final)\n",
    "        return out\n",
    "\n",
    "        #What's actually done:\n",
    "        \"\"\"\n",
    "        we took input (B,64,320,320), passed it in conv1 layer, got (B,128,320,320)\n",
    "\n",
    "        now, we splitted x into x1 and x2\n",
    "\n",
    "        we already had x1 and x2 in outputs[] list each of which is (B,64,320,320)\n",
    "        now we add bottle-necked x1s also num_bneck times, so now our outputs[] list\n",
    "        contains [x1, x2, num_bnecks times modified x1s] and this is why we did\n",
    "        (num_bnecks + 2) in conv2 layer\n",
    "\n",
    "        now we concatenate all these elemnts of list along the channels,\n",
    "        let's say we take 3 bottle necks,\n",
    "        so in output list, we have x1 and x2 (64 channels each)\n",
    "        and 3 more modified x1s (64 channels each)\n",
    "        concatinating them, we get 320 channels\n",
    "        now, these 320 channels --> CONV2 LAYER --> output_channels = 128\n",
    "\n",
    "        SO, AT LAST WE GET A 128-channel IMAGE!!\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cc6d12f9-13dd-4548-93ae-99792f20c17b",
    "_uuid": "a5bbde37-b308-4370-92d1-6d1d2536cb0f",
    "collapsed": false,
    "id": "pl3ngkKqw8AV",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# Just some testing...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "c5c30406-2d3e-435e-b26d-1fac338e8da5",
    "_uuid": "0d18a41c-c0f6-4bfc-84c6-4a3a20aa3ddf",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:42.684859Z",
     "iopub.status.busy": "2025-06-16T11:42:42.684601Z",
     "iopub.status.idle": "2025-06-16T11:42:42.719931Z",
     "shell.execute_reply": "2025-06-16T11:42:42.719261Z",
     "shell.execute_reply.started": "2025-06-16T11:42:42.684835Z"
    },
    "id": "G9LgzHffw8AV",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "image = Image.open(\"/kaggle/input/traffic-data/traffic_wala_dataset/train/images/10_mp4-10_jpg.rf.b87509668caff369c5501325477e6d9a.jpg\").convert(\"RGB\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "image = transform(image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "8d970737-5b94-4a8e-80b0-9dd505d8e01c",
    "_uuid": "a3129ffc-b575-4e61-9720-d0b972ae455b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:42.723161Z",
     "iopub.status.busy": "2025-06-16T11:42:42.722391Z",
     "iopub.status.idle": "2025-06-16T11:42:42.726526Z",
     "shell.execute_reply": "2025-06-16T11:42:42.725876Z",
     "shell.execute_reply.started": "2025-06-16T11:42:42.723142Z"
    },
    "id": "RtUKC2WTw8AV",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "251a40a1-fc0f-4c26-c8bc-9b030973525d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "6caeebcb-b1ff-4d96-b45a-ad9610583235",
    "_uuid": "ef01e8aa-66d6-4527-b057-e7214e45c2e3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:42.727485Z",
     "iopub.status.busy": "2025-06-16T11:42:42.727183Z",
     "iopub.status.idle": "2025-06-16T11:42:42.800243Z",
     "shell.execute_reply": "2025-06-16T11:42:42.799587Z",
     "shell.execute_reply.started": "2025-06-16T11:42:42.727455Z"
    },
    "id": "xpHEP3daw8AV",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "5c926f5f-0f44-4fed-dfe6-39e760321ba6",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 320, 320])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step1 = ConvBNAct(in_channels = 3, out_channels = 64, kernel_size = 3, stride = 2, padding = 1)\n",
    "image2 = step1(image)\n",
    "image2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "6ac19a69-e2fe-4574-9d3b-8d1d1f35f8b7",
    "_uuid": "9ce750e0-b4fb-41e2-b280-9561f5b895bb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:42.801168Z",
     "iopub.status.busy": "2025-06-16T11:42:42.800972Z",
     "iopub.status.idle": "2025-06-16T11:42:42.918790Z",
     "shell.execute_reply": "2025-06-16T11:42:42.918149Z",
     "shell.execute_reply.started": "2025-06-16T11:42:42.801152Z"
    },
    "id": "IGWiSQaOw8AV",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "fa66ae73-977e-4d76-c9a2-667c23a8e0fa",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 320, 320])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step2 = Bottleneck(in_channels = 64, out_channels = 64)\n",
    "image3 = step2(image2)\n",
    "image3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "6d69fbc0-6bb1-4532-9f9c-11fdf59403fc",
    "_uuid": "38784971-ba91-483c-9324-47e05982c5af",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:42.919681Z",
     "iopub.status.busy": "2025-06-16T11:42:42.919467Z",
     "iopub.status.idle": "2025-06-16T11:42:43.688467Z",
     "shell.execute_reply": "2025-06-16T11:42:43.687786Z",
     "shell.execute_reply.started": "2025-06-16T11:42:42.919665Z"
    },
    "id": "OWEtN-9lw8AV",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "5235164c-a07f-4579-a209-0760aad2dec4",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 320, 320])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step3 = c2f(in_channels = 64, out_channels = 128, n = 3, d = 1)\n",
    "image4 = step3(image3)\n",
    "image4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d6f69d44-7212-4d07-a376-e498697ae7b3",
    "_uuid": "fe50dee9-d72b-4eb3-8dd9-231e0ad21e72",
    "collapsed": false,
    "id": "IpmuFSBDw8AV",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Spatial Pyramid Pooling Fast (SPPF) Block:\n",
    "Take features from different \"scales\" and combine them, so that the network becomes better at capturing both local and global features\n",
    "<br>\n",
    "![image.png](images-ref/783560c2-b1f7-4374-a082-afb7b2b2757f.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "0753b1e9-2134-4a72-afad-824fe0bacd1d",
    "_uuid": "bb47733e-0552-4cfe-92d2-ed78111ae0e1",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:43.689449Z",
     "iopub.status.busy": "2025-06-16T11:42:43.689172Z",
     "iopub.status.idle": "2025-06-16T11:42:43.696308Z",
     "shell.execute_reply": "2025-06-16T11:42:43.695693Z",
     "shell.execute_reply.started": "2025-06-16T11:42:43.689424Z"
    },
    "id": "HZ28AgsMw8AV",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class sppf(nn.Module):\n",
    "    def __init__ (self, in_channels, out_channels, expansion = 0.5,num_pool = 3):\n",
    "        super().__init__()\n",
    "        hidden_channels = int(out_channels * expansion)\n",
    "        #1. 1 x 1 convolution\n",
    "        #128 channels --> 64 channels (to reduce the computation)\n",
    "        self.conv1 = ConvBNAct(in_channels, hidden_channels, kernel_size = 1, stride = 1, padding = 0)\n",
    "\n",
    "        #2. Max-Pooling\n",
    "        self.poolLayer = nn.ModuleList([\n",
    "            nn.MaxPool2d(kernel_size = 5, stride = 1, padding = 2) for i in range(num_pool)\n",
    "        ])\n",
    "\n",
    "        #3. 1 x 1 Convolution\n",
    "        #restoring back the desired number of channels...\n",
    "        self.conv2 = ConvBNAct((num_pool + 1)*hidden_channels, out_channels, stride = 1, kernel_size = 1, padding = 0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        outs = [x]\n",
    "\n",
    "        for layer in self.poolLayer:\n",
    "            y1 = layer(x)\n",
    "            outs.append(y1)\n",
    "        x_cat = torch.concat(outs, dim = 1)\n",
    "\n",
    "        out = self.conv2(x_cat)\n",
    "        #print(x_cat.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "94a2e94f-785e-438c-a664-13791c5f05a4",
    "_uuid": "fd46ea29-7114-477a-9bc5-1fca206ed09c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:43.697919Z",
     "iopub.status.busy": "2025-06-16T11:42:43.697417Z",
     "iopub.status.idle": "2025-06-16T11:42:45.732177Z",
     "shell.execute_reply": "2025-06-16T11:42:45.731302Z",
     "shell.execute_reply.started": "2025-06-16T11:42:43.697839Z"
    },
    "id": "34fle7EGw8AV",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "d0ddf8f4-ea9f-44ab-8ec4-026b882b3fee",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 320, 320])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step4 = sppf(in_channels = 128, out_channels = 128, num_pool = 5)\n",
    "image5 = step4(image4)\n",
    "image5.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7e11a4ab-b20c-43c4-a063-f04ff39c37bc",
    "_uuid": "0d286521-e164-4b99-9d1f-69a2c3589ae8",
    "collapsed": false,
    "id": "C3Mscv1Bw8AV",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Putting it Together :- The BACKBONE \n",
    "The backbone is the feature extractor of the model - it takes in the input image and extracts feature maps at multiple scales (from low-level to high-level). These features are then passed to the neck and head for detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "c57dabaa-7432-4c9e-b21d-da468f73ef7a",
    "_uuid": "e80de5e9-e991-4759-bf85-2fb31f9833d1",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:45.733605Z",
     "iopub.status.busy": "2025-06-16T11:42:45.733323Z",
     "iopub.status.idle": "2025-06-16T11:42:45.745038Z",
     "shell.execute_reply": "2025-06-16T11:42:45.744190Z",
     "shell.execute_reply.started": "2025-06-16T11:42:45.733585Z"
    },
    "id": "uX6b8T_2w8AV",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    def __init__(self, depth_mul = 1, width_mul = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        def ch(c):\n",
    "            return max(int(c * float(width_mul)), 1)\n",
    "\n",
    "        def d(n):\n",
    "            return max(round(n * float(depth_mul)), 1)\n",
    "\n",
    "        # Stage-0: Initial Conv layer (P1)\n",
    "        self.stem = ConvBNAct(3, ch(64), 3, 2, 1)\n",
    "\n",
    "        # P2\n",
    "        self.conv1 = ConvBNAct(ch(64), ch(128), 3, 2, 1)\n",
    "        self.stage1 = c2f(ch(128), ch(128), n=d(3), d=1)\n",
    "\n",
    "        # P3\n",
    "        self.conv2 = ConvBNAct(ch(128), ch(256), 3, 2, 1)\n",
    "        self.stage2 = c2f(ch(256), ch(256), n=d(6), d=1)\n",
    "\n",
    "        # P4\n",
    "        self.conv3 = ConvBNAct(ch(256), ch(512), 3, 2, 1)\n",
    "        self.stage3 = c2f(ch(512), ch(512), n=d(6), d=1)\n",
    "\n",
    "        # P5\n",
    "        self.conv4 = ConvBNAct(ch(512), ch(512), 3, 2, 1)\n",
    "        self.stage4 = c2f(ch(512), ch(512), n=d(3), d=1)\n",
    "\n",
    "        # SPPF\n",
    "        self.sppf = sppf(in_channels=ch(512), out_channels=ch(512), num_pool=3)\n",
    "\n",
    "        #Sequential Layer\n",
    "        self.FinalLayer = nn.Sequential(\n",
    "            self.stem,self.conv1, self.stage1,self.conv2, self.stage2,self.conv3, self.stage3,self.conv4, self.stage4,\n",
    "            self.sppf\n",
    "        )\n",
    "\n",
    "        #x1-layer\n",
    "        self.x1_layer = nn.Sequential(\n",
    "            self.stem,self.conv1, self.stage1,self.conv2, self.stage2\n",
    "        )\n",
    "        #x2-layer\n",
    "        self.x2_layer = nn.Sequential(\n",
    "            self.stem,self.conv1, self.stage1,self.conv2, self.stage2,self.conv3, self.stage3\n",
    "        )\n",
    "        #x3-layer\n",
    "        self.x3_layer = nn.Sequential(\n",
    "            self.stem,self.conv1, self.stage1,self.conv2, self.stage2,self.conv3, self.stage3,self.conv4, self.stage4,\n",
    "            self.sppf\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.FinalLayer(x)\n",
    "        x1 = self.x1_layer(x)\n",
    "        x2 = self.x2_layer(x)\n",
    "        x3 = self.x3_layer(x)\n",
    "        return out,x1, x2, x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "a0554d7f-b328-4265-a25c-0c70942a1bcc",
    "_uuid": "6b4c4344-eddb-4070-a514-947ca53432dd",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:45.746357Z",
     "iopub.status.busy": "2025-06-16T11:42:45.746085Z",
     "iopub.status.idle": "2025-06-16T11:42:48.085499Z",
     "shell.execute_reply": "2025-06-16T11:42:48.084626Z",
     "shell.execute_reply.started": "2025-06-16T11:42:45.746338Z"
    },
    "id": "zk1Bia8ew8AW",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "FinalStep = Backbone()\n",
    "out, x1, x2, x3 = FinalStep(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "e5a6c021-4b93-4128-b0f5-a9a1ae9ae25e",
    "_uuid": "d51e4267-f906-4ccc-a73c-8c9e058d278a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:48.086568Z",
     "iopub.status.busy": "2025-06-16T11:42:48.086337Z",
     "iopub.status.idle": "2025-06-16T11:42:48.091546Z",
     "shell.execute_reply": "2025-06-16T11:42:48.090657Z",
     "shell.execute_reply.started": "2025-06-16T11:42:48.086551Z"
    },
    "id": "C1fyQh2xw8AW",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "1c3f6e76-aea2-47d2-81e1-ee5beddd23d0",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Inputs for Neck Layer are======== \n",
      "x1 with shape: torch.Size([1, 256, 80, 80]),\n",
      "x2 with shape: torch.Size([1, 512, 40, 40]) and\n",
      "x3 with shape: torch.Size([1, 512, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "print(f\"========Inputs for Neck Layer are======== \\nx1 with shape: {x1.shape},\\nx2 with shape: {x2.shape} and\\nx3 with shape: {x3.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "14583f9d-89e8-4ceb-a7c5-2dbc7d8781fb",
    "_uuid": "7adcc000-471c-4006-854c-ee94957b92b6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:48.092675Z",
     "iopub.status.busy": "2025-06-16T11:42:48.092444Z",
     "iopub.status.idle": "2025-06-16T11:42:48.116040Z",
     "shell.execute_reply": "2025-06-16T11:42:48.115534Z",
     "shell.execute_reply.started": "2025-06-16T11:42:48.092659Z"
    },
    "id": "nBl-pj9Gw8AW",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "4ffc9966-64d0-41ac-90ab-58f1ffada96c",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.699904 million parameters\n"
     ]
    }
   ],
   "source": [
    "print(f\"{sum(p.numel() for p in FinalStep.parameters())/1e6} million parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "63254f18-9415-4d03-bdc6-f705aeb6024d",
    "_uuid": "dff47f44-5a10-42ba-acdc-7a7ffad3a93e",
    "collapsed": false,
    "id": "NIKVEu1Hw8AW",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## NECK Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "26769134-bcc5-47af-a373-c781a075cf6f",
    "_uuid": "624c7eb8-734f-4d66-90ee-854a417ad8e5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:48.117020Z",
     "iopub.status.busy": "2025-06-16T11:42:48.116807Z",
     "iopub.status.idle": "2025-06-16T11:42:48.133041Z",
     "shell.execute_reply": "2025-06-16T11:42:48.132405Z",
     "shell.execute_reply.started": "2025-06-16T11:42:48.117005Z"
    },
    "id": "XU--48J5w8AW",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class upsample(nn.Module):\n",
    "    def __init__(self, scale_factor = 2, mode = \"nearest\"):\n",
    "        super().__init__()\n",
    "        self.Upsample = nn.Upsample(scale_factor = scale_factor, mode = mode)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.Upsample(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f3a1adf6-9bfd-492a-9160-0eb7385a3acc",
    "_uuid": "dba9545f-5b32-41ca-bc84-a3cfcbe4be73",
    "collapsed": false,
    "id": "mKefN_DNw8AW",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### Testing...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "65b9a479-e870-4bc3-9884-dda64f92329c",
    "_uuid": "8760a015-7193-462f-b9e5-f849013b45a2",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:48.134035Z",
     "iopub.status.busy": "2025-06-16T11:42:48.133811Z",
     "iopub.status.idle": "2025-06-16T11:42:48.150139Z",
     "shell.execute_reply": "2025-06-16T11:42:48.149650Z",
     "shell.execute_reply.started": "2025-06-16T11:42:48.134021Z"
    },
    "id": "CyvQwrf_w8AW",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "493143f0-1a0f-484a-b977-fc9fe631388c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# up = upsample()\n",
    "# u_img = up(x3)\n",
    "# u_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "16162360-1879-4942-8a6d-4bc554fd3ec1",
    "_uuid": "9762c5ce-25b4-4c7f-aeb3-6a1277365fa0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:48.151019Z",
     "iopub.status.busy": "2025-06-16T11:42:48.150803Z",
     "iopub.status.idle": "2025-06-16T11:42:48.169435Z",
     "shell.execute_reply": "2025-06-16T11:42:48.168772Z",
     "shell.execute_reply.started": "2025-06-16T11:42:48.150997Z"
    },
    "id": "6nyST8oGw8AW",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Neck(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.upsample = upsample(scale_factor=2, mode=\"nearest\")\n",
    "\n",
    "        #12 C2f\n",
    "        self.c2f12 = c2f(in_channels = 1024, out_channels = 1024, n = 3, d = 1, shortcut = False)\n",
    "\n",
    "        #15 C2f\n",
    "        self.c2f15 = c2f(in_channels = 768, out_channels = 768, n = 3, d = 1, shortcut = False)\n",
    "\n",
    "        #downsample:\n",
    "        self.down1 = ConvBNAct(in_channels = 768, out_channels = 256, kernel_size = 1, stride = 1, padding = 0)\n",
    "\n",
    "        #16 conv\n",
    "        self.conv16 = ConvBNAct(in_channels = 256, out_channels = 256, kernel_size = 3, stride = 2, padding = 1)\n",
    "\n",
    "        #18 c2f\n",
    "        self.c2f18 = c2f(in_channels = 768, out_channels = 768, n = 3, d = 1, shortcut = False)\n",
    "\n",
    "        #down\n",
    "        self.down2 = ConvBNAct(in_channels = 768, out_channels = 512, kernel_size = 1, stride = 1, padding = 0)\n",
    "\n",
    "        #19 conv\n",
    "        self.conv19 = ConvBNAct(in_channels = 512, out_channels = 512, kernel_size = 3, stride = 2, padding = 1)\n",
    "\n",
    "        #21 c2f\n",
    "        self.c2f21 = c2f(in_channels = 1024, out_channels = 1024, n = 3, d = 1, shortcut = False)\n",
    "\n",
    "        #down\n",
    "        self.down3 = ConvBNAct(in_channels = 1024, out_channels = 512, kernel_size = 1, stride = 1, padding = 0)\n",
    "        self.down4 = ConvBNAct(in_channels = 1024, out_channels = 512, kernel_size = 1, stride = 1, padding = 0)\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "\n",
    "        #10 upsample\n",
    "        x_10 = self.upsample(x3)\n",
    "\n",
    "        #11 Concat\n",
    "        x_11 = torch.concat([x_10, x2], dim = 1)\n",
    "\n",
    "        #12 c2f\n",
    "        x_12 = self.down4(self.c2f12(x_11))\n",
    "\n",
    "        #13 upsample\n",
    "        x_13 = self.upsample(x_12)\n",
    "\n",
    "        #14 concat\n",
    "        x_14 = torch.concat([x_13, x1], dim = 1)\n",
    "\n",
    "        #15 c2f\n",
    "        x_15 = self.c2f15(x_14)\n",
    "        detect_1 = self.down1(x_15)\n",
    "\n",
    "        x_16 = self.conv16(detect_1)\n",
    "\n",
    "        #17 Concat\n",
    "        x_17 = torch.concat([x_12, x_16], dim = 1)\n",
    "\n",
    "        #18\n",
    "        x_18 = self.c2f18(x_17)\n",
    "        detect_2 = self.down2(x_18)\n",
    "        \n",
    "        #19 conv\n",
    "        x_19 = self.conv19(detect_2)\n",
    "\n",
    "        #20Concat\n",
    "        x_20 = torch.concat([x3, x_19], dim = 1)\n",
    "\n",
    "        #21 c2f\n",
    "        x_21 = self.c2f21(x_20)\n",
    "        detect_3 = self.down3(x_21)\n",
    "        return detect_1, detect_2, detect_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "4023ed57-3e62-4a4a-b9d2-521911b7424d",
    "_uuid": "c4418cb5-d639-4c12-9ad5-1f3fc4b8439a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:48.170532Z",
     "iopub.status.busy": "2025-06-16T11:42:48.170322Z",
     "iopub.status.idle": "2025-06-16T11:42:49.412844Z",
     "shell.execute_reply": "2025-06-16T11:42:49.411925Z",
     "shell.execute_reply.started": "2025-06-16T11:42:48.170517Z"
    },
    "id": "Ue8SnDkDw8AW",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "27209118-62bf-47db-aba7-b04b85a9186d",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Outputs for Neck Layer are======== \n",
      "detect_1 with shape: torch.Size([1, 256, 80, 80]),\n",
      "detect_2 with shape: torch.Size([1, 512, 40, 40]),\n",
      "detect_3 with shape: torch.Size([1, 512, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "neck_out = Neck()\n",
    "detect_1, detect_2, detect_3 = neck_out(x1, x2, x3)\n",
    "\n",
    "print(f\"========Outputs for Neck Layer are======== \\n\"\n",
    "      f\"detect_1 with shape: {detect_1.shape},\\n\"\n",
    "      f\"detect_2 with shape: {detect_2.shape},\\n\"\n",
    "      f\"detect_3 with shape: {detect_3.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The DETECT Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "95c4be56-75b2-44ff-bb23-9ae2cb251839",
    "_uuid": "7fe43c55-84ec-493e-a57e-dc53e26112f3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:49.414141Z",
     "iopub.status.busy": "2025-06-16T11:42:49.413864Z",
     "iopub.status.idle": "2025-06-16T11:42:49.421112Z",
     "shell.execute_reply": "2025-06-16T11:42:49.420303Z",
     "shell.execute_reply.started": "2025-06-16T11:42:49.414121Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Detect(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes = 1, reg_max = 15):\n",
    "        super().__init__()\n",
    "        self.reg_max = reg_max\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.bbox_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size = 3, padding = 1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size = 3, padding = 1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(in_channels, 4*(reg_max+1), kernel_size = 1, padding = 0)\n",
    "        )\n",
    "\n",
    "        self.class_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size = 3, padding = 1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size = 3, padding = 1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(in_channels, num_classes, kernel_size = 1, padding = 0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        bounding_box = self.bbox_layer(x)\n",
    "        classif_box = self.class_layer(x)\n",
    "\n",
    "        return bounding_box, classif_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "50bfdcdb-2baa-4538-8a41-2a2be023f5e3",
    "_uuid": "b8365bb0-d625-4526-b87b-3064ba09fa82",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:49.422313Z",
     "iopub.status.busy": "2025-06-16T11:42:49.421988Z",
     "iopub.status.idle": "2025-06-16T11:42:49.440024Z",
     "shell.execute_reply": "2025-06-16T11:42:49.439511Z",
     "shell.execute_reply.started": "2025-06-16T11:42:49.422290Z"
    },
    "id": "xWEI3jEYw8Aa",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(f\"{sum(p.numel() for p in neck_out.parameters())/1e6} million parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "e076e9dc-ca21-468e-8791-0aed5fa781e6",
    "_uuid": "5981639d-cae9-47fe-a794-16f3bf507e54",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:49.440914Z",
     "iopub.status.busy": "2025-06-16T11:42:49.440622Z",
     "iopub.status.idle": "2025-06-16T11:42:49.455802Z",
     "shell.execute_reply": "2025-06-16T11:42:49.455278Z",
     "shell.execute_reply.started": "2025-06-16T11:42:49.440894Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, in_channels_list, num_classes = 1, reg_max = 15):\n",
    "        super().__init__()\n",
    "        self.head_layer = nn.ModuleList([\n",
    "            Detect(in_channels, num_classes, reg_max) for in_channels in in_channels_list\n",
    "            \n",
    "        ])\n",
    "\n",
    "    def forward(self, features):\n",
    "        all_preds = []\n",
    "        for i, feat in enumerate(features):\n",
    "            bbox_pred, cls_pred = self.head_layer[i](feat)\n",
    "            all_preds.append((bbox_pred, cls_pred))\n",
    "        return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "231d2dc9-8800-44af-a8bb-aa9934274b65",
    "_uuid": "17ab2031-6e83-499e-baf8-98cb9cb851c8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:49.456677Z",
     "iopub.status.busy": "2025-06-16T11:42:49.456468Z",
     "iopub.status.idle": "2025-06-16T11:42:50.023081Z",
     "shell.execute_reply": "2025-06-16T11:42:50.022303Z",
     "shell.execute_reply.started": "2025-06-16T11:42:49.456652Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale 1: BBox shape = torch.Size([1, 64, 80, 80]), Cls shape = torch.Size([1, 1, 80, 80])\n",
      "Scale 2: BBox shape = torch.Size([1, 64, 40, 40]), Cls shape = torch.Size([1, 1, 40, 40])\n",
      "Scale 3: BBox shape = torch.Size([1, 64, 20, 20]), Cls shape = torch.Size([1, 1, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "head = Head(in_channels_list=[256,512,512], num_classes=1, reg_max=15)\n",
    "\n",
    "detects = [detect_1, detect_2, detect_3]\n",
    "outputs = head(detects)\n",
    "\n",
    "for i, (bbox, cls) in enumerate(outputs):\n",
    "    print(f\"Scale {i+1}: BBox shape = {bbox.shape}, Cls shape = {cls.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it together: The YOLOv8 Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_cell_guid": "8d09c61e-9da4-4086-9469-96ca03ab8c39",
    "_uuid": "5295794f-c03a-4010-8e56-53c601bbd4c0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:50.027428Z",
     "iopub.status.busy": "2025-06-16T11:42:50.027145Z",
     "iopub.status.idle": "2025-06-16T11:42:50.032035Z",
     "shell.execute_reply": "2025-06-16T11:42:50.031289Z",
     "shell.execute_reply.started": "2025-06-16T11:42:50.027409Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class YOLOv8(nn.Module):\n",
    "    def __init__(self, backbone, neck, head):\n",
    "        super(YOLOv8, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.neck = neck\n",
    "        self.head = head\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: input image tensor (B, 3, H, W)\n",
    "        _, x1, x2, x3 = self.backbone(x)         # Get features from backbone\n",
    "        d1, d2, d3 = self.neck(x1, x2, x3)       # Neck processing\n",
    "        preds = self.head([d1, d2, d3])          # Head predictions\n",
    "        return preds                             # [(bbox1, cls1), (bbox2, cls2), (bbox3, cls3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "363ec657-9c7d-49b1-9e4b-165f0a52ace1",
    "_uuid": "ac075f6e-6d2c-4560-b17f-bac3c1130ddb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:50.033323Z",
     "iopub.status.busy": "2025-06-16T11:42:50.033063Z",
     "iopub.status.idle": "2025-06-16T11:42:50.947119Z",
     "shell.execute_reply": "2025-06-16T11:42:50.946391Z",
     "shell.execute_reply.started": "2025-06-16T11:42:50.033300Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "backbone = Backbone()\n",
    "neck = Neck()  \n",
    "head = Head(in_channels_list=[256, 512, 512], num_classes=1, reg_max=15)\n",
    "model = YOLOv8(backbone, neck, head).to(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just some model testing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "be1f5ab0-bfc6-4f60-81af-618a7ba978c3",
    "_uuid": "115069ee-9091-41c1-b24d-bf18742f2ba5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:50.948580Z",
     "iopub.status.busy": "2025-06-16T11:42:50.947977Z",
     "iopub.status.idle": "2025-06-16T11:42:52.838877Z",
     "shell.execute_reply": "2025-06-16T11:42:52.838278Z",
     "shell.execute_reply.started": "2025-06-16T11:42:50.948556Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale 1:\n",
      " - BBox shape: torch.Size([1, 64, 80, 80])\n",
      " - Cls shape:  torch.Size([1, 1, 80, 80])\n",
      "Scale 2:\n",
      " - BBox shape: torch.Size([1, 64, 40, 40])\n",
      " - Cls shape:  torch.Size([1, 1, 40, 40])\n",
      "Scale 3:\n",
      " - BBox shape: torch.Size([1, 64, 20, 20])\n",
      " - Cls shape:  torch.Size([1, 1, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 640, 640).to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(dummy_input)\n",
    "\n",
    "for i, (bbox, cls) in enumerate(outputs):\n",
    "    print(f\"Scale {i+1}:\")\n",
    "    print(f\" - BBox shape: {bbox.shape}\") \n",
    "    print(f\" - Cls shape:  {cls.shape}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_cell_guid": "5a241a01-6e75-4f92-94e8-a14b1c0cb9e4",
    "_uuid": "819b111a-afe6-4391-8beb-a32c3d89b0ea",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:52.840159Z",
     "iopub.status.busy": "2025-06-16T11:42:52.839876Z",
     "iopub.status.idle": "2025-06-16T11:42:52.847659Z",
     "shell.execute_reply": "2025-06-16T11:42:52.846917Z",
     "shell.execute_reply.started": "2025-06-16T11:42:52.840134Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def decode_bboxes(bbox_pred, stride, reg_max=15):\n",
    "    \n",
    "    B, _, H, W = bbox_pred.shape\n",
    "    # Reshape to [B, 4, reg_max+1, H, W]\n",
    "    bbox_pred = bbox_pred.view(B, 4, reg_max + 1, H, W)\n",
    "\n",
    "    # Softmax over distribution bins\n",
    "    prob = F.softmax(bbox_pred, dim=2)\n",
    "\n",
    "    # Get expected value via projection\n",
    "    proj = torch.arange(reg_max + 1, dtype=prob.dtype, device=prob.device)\n",
    "    dist = (prob * proj[None, None, :, None, None]).sum(dim=2)  # [B, 4, H, W]\n",
    "\n",
    "    # Make grid of anchor centers\n",
    "    grid_y, grid_x = torch.meshgrid(torch.arange(H), torch.arange(W), indexing='ij')\n",
    "    grid = torch.stack((grid_x, grid_y), dim=0).to(bbox_pred.device)  # [2, H, W]\n",
    "\n",
    "    # Convert to image scale\n",
    "    x_center = (grid[0][None] + 0.5) * stride\n",
    "    y_center = (grid[1][None] + 0.5) * stride\n",
    "\n",
    "    # dist = [left, top, right, bottom]\n",
    "    x1 = x_center - dist[:, 0] * stride\n",
    "    y1 = y_center - dist[:, 1] * stride\n",
    "    x2 = x_center + dist[:, 2] * stride\n",
    "    y2 = y_center + dist[:, 3] * stride\n",
    "\n",
    "    xc = (x1 + x2) / 2\n",
    "    yc = (y1 + y2) / 2\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "\n",
    "    bboxes = torch.stack([xc, yc, w, h], dim=-1) \n",
    "    return bboxes.view(B, -1, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "f7cf61c5-5108-498e-a5ec-d9c550e50a51",
    "_uuid": "aabfe883-614c-4d39-b3a5-9754cfc0fb89",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:52.848577Z",
     "iopub.status.busy": "2025-06-16T11:42:52.848394Z",
     "iopub.status.idle": "2025-06-16T11:42:52.874752Z",
     "shell.execute_reply": "2025-06-16T11:42:52.874277Z",
     "shell.execute_reply.started": "2025-06-16T11:42:52.848562Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def process_cls_scores(cls_pred, score_thresh=0.4):\n",
    "    \"\"\"\n",
    "    Process classification predictions for 1-class model.\n",
    "\n",
    "    Args:\n",
    "        cls_pred: Tensor of shape [B, 1, H, W]\n",
    "        score_thresh: minimum score to keep a prediction\n",
    "\n",
    "    Returns:\n",
    "        score_mask: Boolean mask [B, H*W]\n",
    "        scores: Confidence scores [B, H*W]\n",
    "    \"\"\"\n",
    "    B, C, H, W = cls_pred.shape\n",
    "    assert C == 1, \"Only supports 1 class\"\n",
    "\n",
    "    # Apply sigmoid to get confidence scores\n",
    "    probs = torch.sigmoid(cls_pred)  \n",
    "    probs = probs.view(B, -1)        \n",
    "\n",
    "    # Create a boolean mask for scores above threshold\n",
    "    score_mask = probs > score_thresh \n",
    "    \n",
    "    return score_mask, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "99142078-dbe8-4ae5-9a01-b1dc9eac615f",
    "_uuid": "0844ea59-62bf-45f6-91dc-b729969e8010",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:52.875628Z",
     "iopub.status.busy": "2025-06-16T11:42:52.875421Z",
     "iopub.status.idle": "2025-06-16T11:42:52.893838Z",
     "shell.execute_reply": "2025-06-16T11:42:52.893150Z",
     "shell.execute_reply.started": "2025-06-16T11:42:52.875613Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_final_predictions(preds, strides=[8, 16, 32], score_thresh=0.4):\n",
    "    \n",
    "    all_boxes = []\n",
    "    for i, (bbox_pred, cls_pred) in enumerate(preds):\n",
    "        stride = strides[i]\n",
    "        boxes = decode_bboxes(bbox_pred, stride=stride)\n",
    "        conf_mask, confs = process_cls_scores(cls_pred, score_thresh=score_thresh)\n",
    "\n",
    "        # FIXED: Handle batch dimension correctly\n",
    "        for batch_idx in range(boxes.shape[0]):\n",
    "            batch_boxes = boxes[batch_idx]  # [H*W, 4]\n",
    "            batch_mask = conf_mask[batch_idx]  # [H*W]\n",
    "            batch_confs = confs[batch_idx]  # [H*W]\n",
    "            \n",
    "            # Filter valid predictions for this batch\n",
    "            valid_boxes = batch_boxes[batch_mask]\n",
    "            valid_scores = batch_confs[batch_mask]\n",
    "\n",
    "            for j in range(valid_boxes.size(0)):\n",
    "                x, y, w, h = valid_boxes[j]\n",
    "                score = valid_scores[j].item()\n",
    "                all_boxes.append((x.item(), y.item(), w.item(), h.item(), score))\n",
    "\n",
    "    return all_boxes\n",
    "def xywh2xyxy(x):\n",
    "    \n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else torch.tensor(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # x1\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # y1  \n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # x2\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # y2\n",
    "    return y\n",
    "\n",
    "def decode_from_distr(pred, reg_max=15):\n",
    "    \n",
    "    B, channels, H, W = pred.shape\n",
    "    pred = pred.view(B, 4, reg_max + 1, H, W)\n",
    "    \n",
    "    # Apply softmax\n",
    "    prob = F.softmax(pred, dim=2)\n",
    "    \n",
    "    # Create projection weights\n",
    "    proj = torch.arange(reg_max + 1, dtype=prob.dtype, device=prob.device)\n",
    "    \n",
    "    # Calculate expected values\n",
    "    exp = torch.sum(prob * proj[None, None, :, None, None], dim=2)\n",
    "    \n",
    "    # Generate grid coordinates\n",
    "    grid_y, grid_x = torch.meshgrid(torch.arange(H), torch.arange(W), indexing='ij')\n",
    "    grid = torch.stack((grid_x, grid_y), dim=0).to(pred.device)\n",
    "    \n",
    "    # Decode to xyxy format (simplified version)\n",
    "    decoded = exp.view(B, 4, -1).permute(0, 2, 1)  # [B, H*W, 4]\n",
    "    return decoded\n",
    "\n",
    "def get_dfl_targets(boxes, reg_max=15):\n",
    "    \n",
    "    B, N, _ = boxes.shape\n",
    "    targets = torch.zeros(B, N, 4, reg_max + 1, device=boxes.device)\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "03163a06-872b-47b2-a1cb-cc0db24435c2",
    "_uuid": "e509dc98-0cc2-4371-81d9-59da5659dc2e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:52.894726Z",
     "iopub.status.busy": "2025-06-16T11:42:52.894494Z",
     "iopub.status.idle": "2025-06-16T11:42:53.945256Z",
     "shell.execute_reply": "2025-06-16T11:42:53.944451Z",
     "shell.execute_reply.started": "2025-06-16T11:42:52.894703Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box 1: [x=4.0, y=4.1, w=120.0, h=119.3] | score=0.49\n",
      "Box 2: [x=12.0, y=4.1, w=120.0, h=119.3] | score=0.49\n",
      "Box 3: [x=20.0, y=4.1, w=120.0, h=119.3] | score=0.49\n",
      "Box 4: [x=28.0, y=4.1, w=120.0, h=119.3] | score=0.49\n",
      "Box 5: [x=36.0, y=4.1, w=120.0, h=119.3] | score=0.49\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    img = torch.randn(1, 3, 640, 640).to(\"cuda\")  # dummy image\n",
    "    preds = model(img)\n",
    "\n",
    "final_boxes = get_final_predictions(preds, score_thresh=0.4)\n",
    "\n",
    "# Print results\n",
    "for i, (x, y, w, h, score) in enumerate(final_boxes[:5]):\n",
    "    print(f\"Box {i+1}: [x={x:.1f}, y={y:.1f}, w={w:.1f}, h={h:.1f}] | score={score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Max Suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_cell_guid": "f2de779d-c596-436d-8245-9bd9daa40f31",
    "_uuid": "467377e2-67a1-4745-a605-b3a01de9a422",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:53.946482Z",
     "iopub.status.busy": "2025-06-16T11:42:53.946098Z",
     "iopub.status.idle": "2025-06-16T11:42:53.952335Z",
     "shell.execute_reply": "2025-06-16T11:42:53.951442Z",
     "shell.execute_reply.started": "2025-06-16T11:42:53.946455Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torchvision.ops as ops\n",
    "\n",
    "def apply_nms(boxes, scores, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Applies Non-Maximum Suppression to filter overlapping boxes.\n",
    "\n",
    "    Args:\n",
    "        boxes: Tensor of shape (N, 4) in [x_center, y_center, w, h]\n",
    "        scores: Tensor of shape (N,)\n",
    "        iou_threshold: IoU threshold for suppression\n",
    "\n",
    "    Returns:\n",
    "        kept_indices: Tensor of indices of boxes kept after NMS\n",
    "    \"\"\"\n",
    "    if boxes.numel() == 0:\n",
    "        return torch.tensor([], dtype=torch.int64)\n",
    "\n",
    "    # Convert [xc, yc, w, h] → [x1, y1, x2, y2]\n",
    "    x1 = boxes[:, 0] - boxes[:, 2] / 2\n",
    "    y1 = boxes[:, 1] - boxes[:, 3] / 2\n",
    "    x2 = boxes[:, 0] + boxes[:, 2] / 2\n",
    "    y2 = boxes[:, 1] + boxes[:, 3] / 2\n",
    "    boxes_xyxy = torch.stack([x1, y1, x2, y2], dim=1)\n",
    "\n",
    "    keep = ops.nms(boxes_xyxy, scores, iou_threshold=iou_threshold)\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "1ed56b12-79e2-4851-ba4b-f26ecb192191",
    "_uuid": "e748f2c1-aff5-4461-8aa2-2777932a6a81",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:53.953544Z",
     "iopub.status.busy": "2025-06-16T11:42:53.953221Z",
     "iopub.status.idle": "2025-06-16T11:42:54.226262Z",
     "shell.execute_reply": "2025-06-16T11:42:54.225478Z",
     "shell.execute_reply.started": "2025-06-16T11:42:53.953523Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After NMS:\n",
      "Box: [x=624.2, y=624.6, w=478.8, h=476.5] | score=0.51\n",
      "Box: [x=624.2, y=48.6, w=478.8, h=476.5] | score=0.51\n",
      "Box: [x=624.2, y=208.6, w=478.8, h=476.5] | score=0.51\n",
      "Box: [x=624.2, y=368.6, w=478.8, h=476.5] | score=0.51\n",
      "Box: [x=48.2, y=624.6, w=478.8, h=476.5] | score=0.50\n",
      "Box: [x=208.2, y=624.6, w=478.8, h=476.5] | score=0.50\n",
      "Box: [x=368.2, y=624.6, w=478.8, h=476.5] | score=0.50\n",
      "Box: [x=48.2, y=112.6, w=478.8, h=476.5] | score=0.50\n",
      "Box: [x=48.2, y=272.6, w=478.8, h=476.5] | score=0.50\n",
      "Box: [x=48.2, y=432.6, w=478.8, h=476.5] | score=0.50\n"
     ]
    }
   ],
   "source": [
    "# Convert to tensors\n",
    "boxes = torch.tensor([b[:4] for b in final_boxes]).to(\"cuda\")\n",
    "scores = torch.tensor([b[4] for b in final_boxes]).to(\"cuda\")\n",
    "\n",
    "# Apply NMS\n",
    "keep_indices = apply_nms(boxes, scores, iou_threshold=0.5)\n",
    "\n",
    "# Print filtered boxes\n",
    "print(\"After NMS:\")\n",
    "for i in keep_indices[:10]:\n",
    "    x, y, w, h = boxes[i]\n",
    "    score = scores[i]\n",
    "    print(f\"Box: [x={x:.1f}, y={y:.1f}, w={w:.1f}, h={h:.1f}] | score={score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "bac1edbc-6d21-420e-872e-6219d33b0ce2",
    "_uuid": "70297c15-3ece-4331-81f0-0aaf3fef764b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:54.227450Z",
     "iopub.status.busy": "2025-06-16T11:42:54.227165Z",
     "iopub.status.idle": "2025-06-16T11:42:54.238946Z",
     "shell.execute_reply": "2025-06-16T11:42:54.238302Z",
     "shell.execute_reply.started": "2025-06-16T11:42:54.227429Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, images_dir, labels_dir, img_size=640):\n",
    "        self.images_dir = images_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.image_files = sorted([\n",
    "            f for f in os.listdir(images_dir)\n",
    "            if f.endswith(('.jpg', '.png', '.jpeg'))\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.image_files[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_file)\n",
    "        label_path = os.path.join(self.labels_dir, os.path.splitext(img_file)[0] + \".txt\")\n",
    "\n",
    "        # Load image\n",
    "        img = Image.open(img_path).convert(\"RGB\").resize((self.img_size, self.img_size))\n",
    "        img = np.array(img).astype(np.float32) / 255.0\n",
    "        img_tensor = torch.tensor(img).permute(2, 0, 1)  # (C, H, W)\n",
    "\n",
    "        # Load label (class_id cx cy w h)\n",
    "        boxes = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, \"r\") as f:\n",
    "                for line in f.readlines():\n",
    "                    cls, cx, cy, w, h = map(float, line.strip().split())\n",
    "                    boxes.append([cls, cx, cy, w, h])\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        return img_tensor, boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_cell_guid": "42994de9-98f3-4b2f-81f6-f5c6fefd63c4",
    "_uuid": "5d441c65-e8f9-4b8b-b4b0-a7df1d6c9591",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:54.240123Z",
     "iopub.status.busy": "2025-06-16T11:42:54.239869Z",
     "iopub.status.idle": "2025-06-16T11:42:54.260829Z",
     "shell.execute_reply": "2025-06-16T11:42:54.260169Z",
     "shell.execute_reply.started": "2025-06-16T11:42:54.240103Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_datasets():\n",
    "    train_images = \"/kaggle/input/traffic-data/traffic_wala_dataset/train/images\"\n",
    "    train_labels = \"/kaggle/input/traffic-data/traffic_wala_dataset/train/labels\"\n",
    "    val_images = \"/kaggle/input/traffic-data/traffic_wala_dataset/valid/images\"\n",
    "    val_labels = \"/kaggle/input/traffic-data/traffic_wala_dataset/valid/labels\"\n",
    "\n",
    "    train_ds = YOLODataset(train_images, train_labels)\n",
    "    val_ds = YOLODataset(val_images, val_labels)\n",
    "\n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_cell_guid": "50c5f9fa-9aac-46ae-b453-5863e61e88cd",
    "_uuid": "65d0f0fa-50cd-4acb-bc83-1916f95083df",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:54.261831Z",
     "iopub.status.busy": "2025-06-16T11:42:54.261602Z",
     "iopub.status.idle": "2025-06-16T11:42:54.334046Z",
     "shell.execute_reply": "2025-06-16T11:42:54.333401Z",
     "shell.execute_reply.started": "2025-06-16T11:42:54.261812Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 536 training images and 90 validation images.\n",
      "Image shape: torch.Size([3, 640, 640])\n",
      "Boxes: tensor([[0.0000, 0.1336, 0.7797, 0.1375, 0.1945],\n",
      "        [0.0000, 0.3625, 0.7188, 0.1273, 0.2070],\n",
      "        [0.0000, 0.5391, 0.9563, 0.1656, 0.0875],\n",
      "        [0.0000, 0.1344, 0.5953, 0.1375, 0.2438],\n",
      "        [0.0000, 0.2945, 0.5609, 0.0844, 0.1219],\n",
      "        [0.0000, 0.2758, 0.3320, 0.0594, 0.0867],\n",
      "        [0.0000, 0.2203, 0.2516, 0.0594, 0.1047],\n",
      "        [0.0000, 0.2875, 0.1570, 0.0773, 0.1547],\n",
      "        [0.0000, 0.2055, 0.1344, 0.0891, 0.1852],\n",
      "        [0.0000, 0.3688, 0.2117, 0.0461, 0.0648],\n",
      "        [0.0000, 0.6805, 0.3273, 0.0672, 0.0867],\n",
      "        [0.0000, 0.8984, 0.4180, 0.0781, 0.1109],\n",
      "        [0.0000, 0.7312, 0.2180, 0.0477, 0.0773],\n",
      "        [0.0000, 0.6477, 0.2516, 0.0594, 0.0836],\n",
      "        [0.0000, 0.6586, 0.1656, 0.0500, 0.0664],\n",
      "        [0.0000, 0.6727, 0.1000, 0.0359, 0.0586],\n",
      "        [0.0000, 0.7242, 0.0820, 0.0445, 0.0984],\n",
      "        [0.0000, 0.7953, 0.0664, 0.0539, 0.1172],\n",
      "        [0.0000, 0.6430, 0.0453, 0.0312, 0.0508],\n",
      "        [0.0000, 0.5203, 0.1102, 0.0367, 0.0445],\n",
      "        [0.0000, 0.3953, 0.1773, 0.0422, 0.0586]])\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset = create_datasets()\n",
    "print(f\"Loaded {len(train_dataset)} training images and {len(val_dataset)} validation images.\")\n",
    "\n",
    "# Try loading one sample:\n",
    "img, boxes = train_dataset[0]\n",
    "print(\"Image shape:\", img.shape)\n",
    "print(\"Boxes:\", boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_cell_guid": "dc51e9ea-5046-48c0-b195-2c7ad67f0d5e",
    "_uuid": "e0d9fa01-02fd-4c9b-afac-a8051139f208",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:54.335053Z",
     "iopub.status.busy": "2025-06-16T11:42:54.334827Z",
     "iopub.status.idle": "2025-06-16T11:42:54.340036Z",
     "shell.execute_reply": "2025-06-16T11:42:54.339134Z",
     "shell.execute_reply.started": "2025-06-16T11:42:54.335033Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "\n",
    "    for img, target in batch:\n",
    "        images.append(img)\n",
    "        targets.append(target)\n",
    "\n",
    "    images = torch.stack(images, dim=0)  # Stack images (fixed size)\n",
    "    return images, targets               # Keep targets as list (variable size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_cell_guid": "57c27e88-6ea8-4db7-9014-4307e59bd248",
    "_uuid": "7932bfda-b7ab-46d3-ab23-d6aaaeb7a399",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:54.341258Z",
     "iopub.status.busy": "2025-06-16T11:42:54.340872Z",
     "iopub.status.idle": "2025-06-16T11:42:54.424110Z",
     "shell.execute_reply": "2025-06-16T11:42:54.423342Z",
     "shell.execute_reply.started": "2025-06-16T11:42:54.341176Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader working:\n",
      "Images shape: torch.Size([2, 3, 640, 640])\n",
      "Targets length (batch): 2\n",
      "Target sample shape: torch.Size([13, 5])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "NUM_WORKERS = 0 \n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "for batch in train_loader:\n",
    "    images, targets = batch\n",
    "    print(\"Dataloader working:\")\n",
    "    print(\"Images shape:\", images.shape)\n",
    "    print(\"Targets length (batch):\", len(targets))\n",
    "    print(\"Target sample shape:\", targets[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_cell_guid": "7db0ebff-3bac-4958-9bd5-6c5338ed388a",
    "_uuid": "5239c866-400a-445e-9adc-7c8a8e8b8826",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:54.425068Z",
     "iopub.status.busy": "2025-06-16T11:42:54.424868Z",
     "iopub.status.idle": "2025-06-16T11:42:54.431730Z",
     "shell.execute_reply": "2025-06-16T11:42:54.430963Z",
     "shell.execute_reply.started": "2025-06-16T11:42:54.425052Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=0.25, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        ce_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        p_t = targets * probs + (1 - targets) * (1 - probs)\n",
    "        loss = ce_loss * ((1 - p_t) ** self.gamma)\n",
    "\n",
    "        if self.alpha >= 0:\n",
    "            alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "            loss *= alpha_t\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_cell_guid": "84af5fef-94a8-47a2-9d33-fe0ddd62507e",
    "_uuid": "41017346-2ee3-4b43-bb06-4899aa7d09ee",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:54.433133Z",
     "iopub.status.busy": "2025-06-16T11:42:54.432629Z",
     "iopub.status.idle": "2025-06-16T11:42:54.456989Z",
     "shell.execute_reply": "2025-06-16T11:42:54.456415Z",
     "shell.execute_reply.started": "2025-06-16T11:42:54.433113Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DistributionFocalLoss(nn.Module):\n",
    "    def __init__(self, reg_max=15):\n",
    "        super().__init__()\n",
    "        self.reg_max = reg_max\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # FIXED: Added clamping to prevent index overflow\n",
    "        dis_left = target.long()\n",
    "        dis_right = torch.clamp(dis_left + 1, max=self.reg_max)\n",
    "\n",
    "        weight_right = target - dis_left\n",
    "        weight_left = 1 - weight_right\n",
    "\n",
    "        pred = pred.log_softmax(dim=-1)\n",
    "        loss_left = F.nll_loss(pred.view(-1, pred.shape[-1]), dis_left.view(-1), reduction='none')\n",
    "        loss_right = F.nll_loss(pred.view(-1, pred.shape[-1]), dis_right.view(-1), reduction='none')\n",
    "\n",
    "        loss = (weight_left.view(-1) * loss_left + weight_right.view(-1) * loss_right).view(target.shape)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_cell_guid": "62f18bb6-2331-4bb9-ad3f-668f990246ae",
    "_uuid": "24456332-1ab6-4553-967a-c30cba811e18",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:54.457940Z",
     "iopub.status.busy": "2025-06-16T11:42:54.457719Z",
     "iopub.status.idle": "2025-06-16T11:42:54.474771Z",
     "shell.execute_reply": "2025-06-16T11:42:54.474106Z",
     "shell.execute_reply.started": "2025-06-16T11:42:54.457921Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def bbox_iou(box1, box2, eps=1e-7):\n",
    "    # box: [x1, y1, x2, y2]\n",
    "    x1 = torch.max(box1[:, 0], box2[:, 0])\n",
    "    y1 = torch.max(box1[:, 1], box2[:, 1])\n",
    "    x2 = torch.min(box1[:, 2], box2[:, 2])\n",
    "    y2 = torch.min(box1[:, 3], box2[:, 3])\n",
    "\n",
    "    inter = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "    area1 = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n",
    "    area2 = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n",
    "\n",
    "    union = area1 + area2 - inter + eps\n",
    "    iou = inter / union\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_cell_guid": "a35899b0-6450-4ffa-b3e8-69d44e773dd4",
    "_uuid": "b4ec5c83-1a91-4e71-a494-9a838037292d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:54.475779Z",
     "iopub.status.busy": "2025-06-16T11:42:54.475579Z",
     "iopub.status.idle": "2025-06-16T11:42:54.498896Z",
     "shell.execute_reply": "2025-06-16T11:42:54.498162Z",
     "shell.execute_reply.started": "2025-06-16T11:42:54.475764Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_targets(targets, batch_size, device, image_size=640):\n",
    "    \"\"\"Convert YOLO format targets to [x1, y1, x2, y2] in pixel coordinates.\"\"\"\n",
    "    max_objects = max(len(t) for t in targets) if targets and any(len(t) > 0 for t in targets) else 1\n",
    "\n",
    "    gt_labels = torch.zeros(batch_size, max_objects, 1, device=device)\n",
    "    gt_bboxes = torch.zeros(batch_size, max_objects, 4, device=device)\n",
    "    mask_gt = torch.zeros(batch_size, max_objects, 1, device=device)\n",
    "\n",
    "    for i, target in enumerate(targets):\n",
    "        if len(target) > 0:\n",
    "            target = target.to(device)\n",
    "\n",
    "            \n",
    "            xywh = target[:, 1:5].clone()\n",
    "            xywh[:, 0] *= image_size  # cx\n",
    "            xywh[:, 1] *= image_size  # cy\n",
    "            xywh[:, 2] *= image_size  # w\n",
    "            xywh[:, 3] *= image_size  # h\n",
    "\n",
    "            \n",
    "            x1 = xywh[:, 0:1] - xywh[:, 2:3] / 2\n",
    "            y1 = xywh[:, 1:2] - xywh[:, 3:4] / 2\n",
    "            x2 = xywh[:, 0:1] + xywh[:, 2:3] / 2\n",
    "            y2 = xywh[:, 1:2] + xywh[:, 3:4] / 2\n",
    "            xyxy = torch.cat([x1, y1, x2, y2], dim=1)\n",
    "\n",
    "            gt_labels[i, :len(target), 0] = target[:, 0]\n",
    "            gt_bboxes[i, :len(target)] = xyxy\n",
    "            mask_gt[i, :len(target), 0] = 1\n",
    "\n",
    "    return gt_labels, gt_bboxes, mask_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_cell_guid": "352ec69b-08eb-46c9-a597-fe25f107a0e0",
    "_uuid": "c09a1642-c5de-448e-9a38-14c950eb6ef5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:54.500350Z",
     "iopub.status.busy": "2025-06-16T11:42:54.499866Z",
     "iopub.status.idle": "2025-06-16T11:42:54.519674Z",
     "shell.execute_reply": "2025-06-16T11:42:54.519031Z",
     "shell.execute_reply.started": "2025-06-16T11:42:54.500325Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def bbox_ciou_loss(pred_boxes, target_boxes, eps=1e-7):\n",
    "    \"\"\"\n",
    "    pred_boxes, target_boxes: [N, 4] in xyxy format\n",
    "    \"\"\"\n",
    "    x1 = torch.max(pred_boxes[:, 0], target_boxes[:, 0])\n",
    "    y1 = torch.max(pred_boxes[:, 1], target_boxes[:, 1])\n",
    "    x2 = torch.min(pred_boxes[:, 2], target_boxes[:, 2])\n",
    "    y2 = torch.min(pred_boxes[:, 3], target_boxes[:, 3])\n",
    "\n",
    "    inter = (x2 - x1).clamp(min=0) * (y2 - y1).clamp(min=0)\n",
    "    area_pred = (pred_boxes[:, 2] - pred_boxes[:, 0]) * (pred_boxes[:, 3] - pred_boxes[:, 1])\n",
    "    area_target = (target_boxes[:, 2] - target_boxes[:, 0]) * (target_boxes[:, 3] - target_boxes[:, 1])\n",
    "    union = area_pred + area_target - inter + eps\n",
    "    iou = inter / union\n",
    "\n",
    "    # Center distance\n",
    "    px = (pred_boxes[:, 0] + pred_boxes[:, 2]) / 2\n",
    "    py = (pred_boxes[:, 1] + pred_boxes[:, 3]) / 2\n",
    "    gx = (target_boxes[:, 0] + target_boxes[:, 2]) / 2\n",
    "    gy = (target_boxes[:, 1] + target_boxes[:, 3]) / 2\n",
    "    center_dist = (px - gx) ** 2 + (py - gy) ** 2\n",
    "\n",
    "    # Enclosing box diagonal\n",
    "    x1_c = torch.min(pred_boxes[:, 0], target_boxes[:, 0])\n",
    "    y1_c = torch.min(pred_boxes[:, 1], target_boxes[:, 1])\n",
    "    x2_c = torch.max(pred_boxes[:, 2], target_boxes[:, 2])\n",
    "    y2_c = torch.max(pred_boxes[:, 3], target_boxes[:, 3])\n",
    "    diagonal = ((x2_c - x1_c) ** 2 + (y2_c - y1_c) ** 2) + eps\n",
    "\n",
    "    ciou = iou - (center_dist / diagonal)\n",
    "    loss = 1.0 - ciou.clamp(min=-1.0, max=1.0)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_cell_guid": "002fe58d-fab8-4454-bf31-2b37f3f17da8",
    "_uuid": "490745d7-5c57-4414-b197-ee2bcd92705c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:54.520723Z",
     "iopub.status.busy": "2025-06-16T11:42:54.520463Z",
     "iopub.status.idle": "2025-06-16T11:42:54.540655Z",
     "shell.execute_reply": "2025-06-16T11:42:54.539989Z",
     "shell.execute_reply.started": "2025-06-16T11:42:54.520702Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_anchors(feats, strides):\n",
    "    \"\"\"Generate anchor points.\"\"\"\n",
    "    anchor_points, stride_tensor = [], []\n",
    "\n",
    "    for i, stride in enumerate(strides):\n",
    "        h, w = feats[i].shape[2:]\n",
    "        y, x = torch.arange(h, dtype=torch.float32), torch.arange(w, dtype=torch.float32)\n",
    "        yv, xv = torch.meshgrid(y, x, indexing='ij')\n",
    "\n",
    "        grid = torch.stack([xv, yv], 2).view(-1, 2)\n",
    "        anchor_points.append(grid * stride + stride / 2)\n",
    "        stride_tensor.append(torch.full((grid.shape[0], 1), stride, dtype=torch.float32))\n",
    "\n",
    "    anchor_points = torch.cat(anchor_points, 0).to(feats[0].device)\n",
    "    stride_tensor = torch.cat(stride_tensor, 0).to(feats[0].device)\n",
    "\n",
    "    return anchor_points, stride_tensor\n",
    "\n",
    "def decode_bboxes_dfl(bbox_pred, anchor_points, stride_tensor, reg_max=15):\n",
    "    \"\"\"Decode bbox predictions from DFL format.\"\"\"\n",
    "    batch_size, num_anchors, _ = bbox_pred.shape\n",
    "    bbox_pred = bbox_pred.view(batch_size, num_anchors, 4, reg_max + 1)\n",
    "    bbox_pred = F.softmax(bbox_pred, dim=-1)\n",
    "\n",
    "    proj = torch.arange(reg_max + 1, dtype=bbox_pred.dtype, device=bbox_pred.device)\n",
    "    bbox_pred = (bbox_pred * proj[None, None, None, :]).sum(dim=-1)\n",
    "\n",
    "    lt, rb = bbox_pred.chunk(2, dim=-1)\n",
    "    anchor_points = anchor_points[None].expand(batch_size, -1, -1)\n",
    "    stride_tensor = stride_tensor[None].expand(batch_size, -1, -1)\n",
    "\n",
    "    x1y1 = anchor_points - lt * stride_tensor\n",
    "    x2y2 = anchor_points + rb * stride_tensor\n",
    "\n",
    "    return torch.cat([x1y1, x2y2], dim=-1)\n",
    "\n",
    "def process_predictions(outputs, strides=[8, 16, 32]):\n",
    "    \"\"\"Convert YOLOv8 outputs to loss-compatible format.\"\"\"\n",
    "    pred_scores_list, pred_bboxes_list, feats = [], [], []\n",
    "\n",
    "    for (bbox_pred, cls_pred) in outputs:\n",
    "        batch_size, _, h, w = cls_pred.shape\n",
    "        cls_pred_flat = cls_pred.view(batch_size, -1, h * w).transpose(1, 2)\n",
    "        bbox_pred_flat = bbox_pred.view(batch_size, -1, h * w).transpose(1, 2)\n",
    "\n",
    "        pred_scores_list.append(cls_pred_flat)\n",
    "        pred_bboxes_list.append(bbox_pred_flat)\n",
    "        feats.append(cls_pred)\n",
    "\n",
    "    pred_scores = torch.cat(pred_scores_list, dim=1)\n",
    "    pred_bboxes_raw = torch.cat(pred_bboxes_list, dim=1)\n",
    "\n",
    "    anchor_points, stride_tensor = make_anchors(feats, strides)\n",
    "    pred_bboxes = decode_bboxes_dfl(pred_bboxes_raw, anchor_points, stride_tensor)\n",
    "\n",
    "    return pred_scores, pred_bboxes, anchor_points, stride_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_cell_guid": "66e118a2-e6a4-47fd-85ba-1cddb9224380",
    "_uuid": "b92e7447-530c-42ef-a6ad-faac0d5b5e00",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:54.541813Z",
     "iopub.status.busy": "2025-06-16T11:42:54.541476Z",
     "iopub.status.idle": "2025-06-16T11:42:54.562965Z",
     "shell.execute_reply": "2025-06-16T11:42:54.562278Z",
     "shell.execute_reply.started": "2025-06-16T11:42:54.541791Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SimpleTaskAlignedAssigner(nn.Module):\n",
    "    def __init__(self, topk=13, num_classes=1, alpha=1.0, beta=6.0):\n",
    "        super().__init__()\n",
    "        self.topk, self.alpha, self.beta = topk, alpha, beta\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def compute_iou(self, pred_bboxes, gt_bboxes):\n",
    "        pred_bboxes = pred_bboxes[:, None, :]  \n",
    "        gt_bboxes = gt_bboxes[None, :, :]      \n",
    "\n",
    "        lt = torch.max(pred_bboxes[..., :2], gt_bboxes[..., :2])\n",
    "        rb = torch.min(pred_bboxes[..., 2:], gt_bboxes[..., 2:])\n",
    "\n",
    "        wh = (rb - lt).clamp(min=0)\n",
    "        inter = wh[..., 0] * wh[..., 1]\n",
    "\n",
    "        pred_area = (pred_bboxes[..., 2] - pred_bboxes[..., 0]) * (pred_bboxes[..., 3] - pred_bboxes[..., 1])\n",
    "        gt_area = (gt_bboxes[..., 2] - gt_bboxes[..., 0]) * (gt_bboxes[..., 3] - gt_bboxes[..., 1])\n",
    "\n",
    "        union = pred_area + gt_area - inter\n",
    "        iou = inter / (union + 1e-6)\n",
    "\n",
    "        return iou\n",
    "\n",
    "    def forward(self, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt):\n",
    "        batch_size, num_anchors = pd_scores.shape[:2]\n",
    "        device = pd_scores.device\n",
    "\n",
    "        target_labels = torch.zeros(batch_size, num_anchors, dtype=torch.float, device=device)\n",
    "        target_bboxes = torch.zeros(batch_size, num_anchors, 4, device=device)\n",
    "        target_scores = torch.zeros(batch_size, num_anchors, self.num_classes, device=device)\n",
    "        fg_mask = torch.zeros(batch_size, num_anchors, dtype=torch.bool, device=device)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            valid_gt = mask_gt[b].squeeze(-1).bool()\n",
    "            num_gt = valid_gt.sum()\n",
    "\n",
    "            if num_gt == 0:\n",
    "                continue\n",
    "\n",
    "            gt_labels_b = gt_labels[b][valid_gt].squeeze(-1)\n",
    "            gt_bboxes_b = gt_bboxes[b][valid_gt]\n",
    "\n",
    "            ious = self.compute_iou(pd_bboxes[b], gt_bboxes_b)\n",
    "            cls_scores = pd_scores[b][:, gt_labels_b.long()]\n",
    "            align_metric = cls_scores.sigmoid() ** self.alpha * ious ** self.beta\n",
    "\n",
    "            topk_vals, topk_idxs = align_metric.topk(min(self.topk, num_anchors), dim=0)\n",
    "\n",
    "            # Replace line 816-817 in SimpleTaskAlignedAssigner.forward()\n",
    "            for gt_idx in range(num_gt):\n",
    "                anchor_idxs = topk_idxs[:, gt_idx]\n",
    "                for k, anchor_idx in enumerate(anchor_idxs):\n",
    "                    fg_mask[b, anchor_idx] = True\n",
    "                    target_labels[b, anchor_idx] = gt_labels_b[gt_idx]\n",
    "                    target_bboxes[b, anchor_idx] = gt_bboxes_b[gt_idx]\n",
    "            \n",
    "                   \n",
    "                    score_sum = topk_vals[:, gt_idx].sum()\n",
    "                    norm_score = topk_vals[k, gt_idx] / (score_sum + 1e-6)\n",
    "            \n",
    "                    class_idx = int(gt_labels_b[gt_idx].item())\n",
    "                    target_scores[b, anchor_idx, class_idx] = norm_score\n",
    "            \n",
    "                    class_idx = int(gt_labels_b[gt_idx].item())\n",
    "                    target_scores[b, anchor_idx, class_idx] = topk_vals[k, gt_idx]\n",
    "\n",
    "        return target_labels, target_bboxes, target_scores, fg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_cell_guid": "c0ebc3aa-1d7c-459a-ab3b-1dc625dded7d",
    "_uuid": "16a9c150-5eaf-40ff-a3ca-37b0ca7af759",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:54.563902Z",
     "iopub.status.busy": "2025-06-16T11:42:54.563682Z",
     "iopub.status.idle": "2025-06-16T11:42:54.585973Z",
     "shell.execute_reply": "2025-06-16T11:42:54.585382Z",
     "shell.execute_reply.started": "2025-06-16T11:42:54.563887Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_val_loss(focal_loss, pred_scores, pred_bboxes, gt_labels, gt_bboxes, mask_gt):\n",
    "    assigner = SimpleTaskAlignedAssigner(topk=10, num_classes=1)  # 🔧 Define it here\n",
    "\n",
    "    _, target_bboxes, target_scores, fg_mask = assigner(\n",
    "        pred_scores, pred_bboxes, None, gt_labels, gt_bboxes, mask_gt\n",
    "    )\n",
    "\n",
    "    if fg_mask.sum() > 0:\n",
    "        cls_loss = focal_loss(pred_scores[fg_mask], target_scores[fg_mask])\n",
    "        bbox_loss = 1.0 - bbox_iou(pred_bboxes[fg_mask], target_bboxes[fg_mask]).mean()\n",
    "    else:\n",
    "        background_targets = torch.zeros_like(pred_scores)\n",
    "        cls_loss = focal_loss(pred_scores, background_targets) * 0.1\n",
    "        bbox_loss = torch.tensor(0.0, device=pred_scores.device)\n",
    "\n",
    "    return cls_loss, bbox_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_cell_guid": "3de41035-cf81-40f9-86bb-30dd22bd6ae7",
    "_uuid": "e02e6e6f-84a8-4622-a949-412e21ef7a8b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:54.586912Z",
     "iopub.status.busy": "2025-06-16T11:42:54.586717Z",
     "iopub.status.idle": "2025-06-16T11:42:54.603871Z",
     "shell.execute_reply": "2025-06-16T11:42:54.603259Z",
     "shell.execute_reply.started": "2025-06-16T11:42:54.586897Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, device, num_epochs=10, lr=1e-4):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    focal_loss = FocalLoss()\n",
    "    assigner = SimpleTaskAlignedAssigner(topk=10, num_classes=1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx, (imgs, targets) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "            imgs = imgs.to(device)\n",
    "            outputs = model(imgs)\n",
    "            batch_size = imgs.shape[0]\n",
    "        \n",
    "            pred_scores, pred_bboxes, anchor_points, _ = process_predictions(outputs)\n",
    "            gt_labels, gt_bboxes, mask_gt = preprocess_targets(targets, batch_size, device)\n",
    "        \n",
    "            _, target_bboxes, target_scores, fg_mask = assigner(\n",
    "                pred_scores, pred_bboxes, anchor_points, gt_labels, gt_bboxes, mask_gt\n",
    "            )\n",
    "        \n",
    "            # print(f\"[DEBUG] Epoch {epoch+1}, Batch {batch_idx}:\")\n",
    "            # print(f\"  - Matched foreground anchors: {fg_mask.sum().item()}\")\n",
    "            # print(f\"  - GT boxes: {gt_bboxes[0][mask_gt[0].squeeze(-1).bool()]}\")\n",
    "            # print(f\"  - Max pred score: {pred_scores.sigmoid().max().item():.4f}\")\n",
    "\n",
    "            print(f\"[Epoch {epoch+1}] Foreground matches: {fg_mask.sum().item()} / {fg_mask.numel()} ({100 * fg_mask.sum().item() / fg_mask.numel():.2f}%)\")\n",
    "\n",
    "            # Compute loss\n",
    "            if fg_mask.sum() > 0:\n",
    "                cls_loss = focal_loss(pred_scores[fg_mask], target_scores[fg_mask])\n",
    "                bbox_loss = 1.0 - bbox_iou(pred_bboxes[fg_mask], target_bboxes[fg_mask]).mean()\n",
    "                bg_loss = focal_loss(pred_scores[~fg_mask], torch.zeros_like(pred_scores[~fg_mask])) * 0.1\n",
    "                loss = cls_loss + bbox_loss + bg_loss  # Combined loss\n",
    "            else:\n",
    "                background_targets = torch.zeros_like(pred_scores)\n",
    "                bg_weight = 0.1  # Reduced background loss weight\n",
    "                bg_targets = torch.zeros_like(pred_scores)\n",
    "                loss = focal_loss(pred_scores, torch.zeros_like(pred_scores)) * 0.1\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        val_loss = validate(model, val_loader, device, lambda *args: compute_val_loss(focal_loss, *args))\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {total_loss / len(train_loader):.4f} | Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_cell_guid": "620a6b71-ea37-4913-925d-ef53f35c7941",
    "_uuid": "1b889889-4ea2-48a0-9919-6dd7eaa5b63a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:42:54.605449Z",
     "iopub.status.busy": "2025-06-16T11:42:54.604597Z",
     "iopub.status.idle": "2025-06-16T11:42:54.629535Z",
     "shell.execute_reply": "2025-06-16T11:42:54.628850Z",
     "shell.execute_reply.started": "2025-06-16T11:42:54.605415Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, val_loader, device, loss_fn):\n",
    "    model.eval()\n",
    "    total_cls_loss = 0.0\n",
    "    total_box_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for images, targets in val_loader:\n",
    "        images = images.to(device)\n",
    "        batch_size = images.shape[0]\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Step 1 & 2: Process preds and targets\n",
    "        pred_scores, pred_bboxes, anchor_points, _ = process_predictions(outputs)\n",
    "        gt_labels, gt_bboxes, mask_gt = preprocess_targets(targets, batch_size, device)\n",
    "\n",
    "        # Compute losses\n",
    "        cls_loss, bbox_loss = loss_fn(\n",
    "            pred_scores, pred_bboxes, gt_labels, gt_bboxes, mask_gt\n",
    "        )\n",
    "\n",
    "        total_cls_loss += cls_loss.item() * batch_size\n",
    "        total_box_loss += bbox_loss.item() * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "    avg_cls = total_cls_loss / total_samples\n",
    "    avg_bbox = total_box_loss / total_samples\n",
    "    avg_total = avg_cls + avg_bbox\n",
    "\n",
    "    print(f\"Validation Loss → Total: {avg_total:.4f}, CLS: {avg_cls:.4f}, BBOX: {avg_bbox:.4f}\")\n",
    "    return avg_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_cell_guid": "c80ef10d-b282-4934-9b0e-e6e4d1bdb8ad",
    "_uuid": "9b8a488d-aeef-48fb-803e-a59d5ae15e01",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:43:02.488757Z",
     "iopub.status.busy": "2025-06-16T11:43:02.488469Z",
     "iopub.status.idle": "2025-06-16T11:43:02.653124Z",
     "shell.execute_reply": "2025-06-16T11:43:02.652557Z",
     "shell.execute_reply.started": "2025-06-16T11:43:02.488735Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_cell_guid": "6a1a8c87-322d-403f-a2e7-510203ff1fb5",
    "_uuid": "ea472b97-fa1c-4d6d-aa2a-8b52eab2d384",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-06-16T11:43:02.699183Z",
     "iopub.status.busy": "2025-06-16T11:43:02.698708Z",
     "iopub.status.idle": "2025-06-16T11:43:02.702734Z",
     "shell.execute_reply": "2025-06-16T11:43:02.702063Z",
     "shell.execute_reply.started": "2025-06-16T11:43:02.699161Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b06b70a1-e9fd-4637-bbf0-8c1a00626a5f",
    "_uuid": "efbcbe57-f5f0-42e2-8a9c-40d9aef3a76e",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader, \n",
    "    device=device,\n",
    "    num_epochs=10,\n",
    "    lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f77f820d-bd52-42f9-96f3-dee048138c04",
    "_uuid": "e7776388-ba0d-4a3b-bf0a-f0480eef49a2",
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2025-06-16T11:42:54.684373Z",
     "iopub.status.idle": "2025-06-16T11:42:54.684684Z",
     "shell.execute_reply": "2025-06-16T11:42:54.684543Z",
     "shell.execute_reply.started": "2025-06-16T11:42:54.684528Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the full model (architecture + weights)\n",
    "torch.save(model, \"yolov8_custom_vehicle_1.pt\")\n",
    "print(\"Model saved as yolov8_custom_vehicle_1.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "\n",
    "# Load your trained model\n",
    "model = torch.load(\"/kaggle/working/yolov8_custom_vehicle_1.pt\", weights_only=False).eval().cuda()\n",
    "\n",
    "# Load and preprocess image\n",
    "img = cv2.imread(\"/kaggle/input/traffic-data/traffic_wala_dataset/valid/images/10_mp4-0_jpg.rf.08b3bd34bbb73fb80c2d662c34474a98.jpg\")[..., ::-1]  # replace with your image path\n",
    "img_resized = cv2.resize(img, (640, 640)) / 255.0\n",
    "img_tensor = torch.tensor(img_resized).permute(2, 0, 1).unsqueeze(0).float().cuda()\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    preds = model(img_tensor)\n",
    "\n",
    "# Print classification confidence for each scale\n",
    "for i, (bbox, cls) in enumerate(preds):\n",
    "    print(f\"Scale {i+1}:\")\n",
    "    print(f\"  BBox Shape: {bbox.shape}\")\n",
    "    print(f\"  Cls Shape : {cls.shape}\")\n",
    "    print(f\"  Cls Max   : {cls.sigmoid().max().item():.4f}\")\n",
    "    print(f\"  Cls Mean  : {cls.sigmoid().mean().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-16T11:42:54.687172Z",
     "iopub.status.idle": "2025-06-16T11:42:54.687504Z",
     "shell.execute_reply": "2025-06-16T11:42:54.687359Z",
     "shell.execute_reply.started": "2025-06-16T11:42:54.687344Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def predict_yolo_labels(image_path, model, score_thresh=0.3, input_size=640, reg_max=15):\n",
    "    \"\"\"\n",
    "    Run inference on a single image and return YOLO-format labels:\n",
    "    Format: [class_id x_center y_center width height] (normalized to [0,1])\n",
    "    \"\"\"\n",
    "    # Load and preprocess image\n",
    "    img = cv2.imread(image_path)[..., ::-1]  # BGR to RGB\n",
    "    orig_h, orig_w = img.shape[:2]\n",
    "    img_resized = cv2.resize(img, (input_size, input_size)) / 255.0\n",
    "    img_tensor = torch.tensor(img_resized).permute(2, 0, 1).unsqueeze(0).float().cuda()\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        preds = model(img_tensor)\n",
    "\n",
    "    pred_boxes_all, pred_scores_all = [], []\n",
    "    strides = [8, 16, 32]\n",
    "\n",
    "    for i, (bbox_raw, cls_raw) in enumerate(preds):\n",
    "        stride = strides[i]\n",
    "        B, _, H, W = cls_raw.shape\n",
    "        cls_conf = cls_raw.sigmoid().squeeze(0).flatten()  # [H*W]\n",
    "\n",
    "        keep = cls_conf > score_thresh\n",
    "        if keep.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        # Decode DFL box predictions\n",
    "        bbox_raw = bbox_raw.view(1, 4, reg_max + 1, H, W)\n",
    "        prob = F.softmax(bbox_raw, dim=2)\n",
    "        proj = torch.arange(reg_max + 1, device=prob.device, dtype=torch.float32)\n",
    "        dist = (prob * proj[None, None, :, None, None]).sum(dim=2)  # [1, 4, H, W]\n",
    "        dist = dist.squeeze(0).permute(1, 2, 0).reshape(-1, 4)  # [H*W, 4]\n",
    "\n",
    "        # Grid centers\n",
    "        grid_y, grid_x = torch.meshgrid(torch.arange(H), torch.arange(W), indexing='ij')\n",
    "        grid = torch.stack([grid_x, grid_y], dim=-1).reshape(-1, 2).float().to(dist.device)\n",
    "\n",
    "        # Convert distances to boxes (xywh)\n",
    "        x_center = (grid[:, 0] + 0.5) * stride\n",
    "        y_center = (grid[:, 1] + 0.5) * stride\n",
    "        x1 = x_center - dist[:, 0] * stride\n",
    "        y1 = y_center - dist[:, 1] * stride\n",
    "        x2 = x_center + dist[:, 2] * stride\n",
    "        y2 = y_center + dist[:, 3] * stride\n",
    "\n",
    "        xc = (x1 + x2) / 2\n",
    "        yc = (y1 + y2) / 2\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1\n",
    "\n",
    "        boxes = torch.stack([xc, yc, w, h], dim=1)  # [N, 4]\n",
    "\n",
    "        # Keep only confident predictions\n",
    "        boxes = boxes[keep]\n",
    "        scores = cls_conf[keep]\n",
    "\n",
    "        pred_boxes_all.append(boxes)\n",
    "        pred_scores_all.append(scores)\n",
    "\n",
    "    if not pred_boxes_all:\n",
    "        return []\n",
    "\n",
    "    final_boxes = torch.cat(pred_boxes_all, dim=0)\n",
    "    final_scores = torch.cat(pred_scores_all, dim=0)\n",
    "\n",
    "    # Normalize to [0,1]\n",
    "    final_boxes[:, 0] /= input_size\n",
    "    final_boxes[:, 1] /= input_size\n",
    "    final_boxes[:, 2] /= input_size\n",
    "    final_boxes[:, 3] /= input_size\n",
    "    final_boxes = final_boxes.clamp(0, 1)\n",
    "\n",
    "    # Format as YOLO labels\n",
    "    yolo_labels = []\n",
    "    for i in range(final_boxes.shape[0]):\n",
    "        xc, yc, w, h = final_boxes[i]\n",
    "        yolo_labels.append(f\"0 {xc:.6f} {yc:.6f} {w:.6f} {h:.6f}\")\n",
    "\n",
    "    return yolo_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-16T11:42:54.688571Z",
     "iopub.status.idle": "2025-06-16T11:42:54.688905Z",
     "shell.execute_reply": "2025-06-16T11:42:54.688759Z",
     "shell.execute_reply.started": "2025-06-16T11:42:54.688742Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "labels = predict_yolo_labels(\"/kaggle/input/traffic-data/traffic_wala_dataset/valid/images/10_mp4-0_jpg.rf.08b3bd34bbb73fb80c2d662c34474a98.jpg\", model, score_thresh=0.1)\n",
    "\n",
    "print(\"YOLO Format Predictions:\")\n",
    "print(\"\\n\".join(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-16T11:42:54.689894Z",
     "iopub.status.idle": "2025-06-16T11:42:54.690167Z",
     "shell.execute_reply": "2025-06-16T11:42:54.690046Z",
     "shell.execute_reply.started": "2025-06-16T11:42:54.690033Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "def predict_yolo_labels(image_path, model, score_thresh=0.3, input_size=640, reg_max=15):\n",
    "    img = cv2.imread(image_path)[..., ::-1]  # BGR to RGB\n",
    "    img_resized = cv2.resize(img, (input_size, input_size)) / 255.0\n",
    "    img_tensor = torch.tensor(img_resized).permute(2, 0, 1).unsqueeze(0).float().cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(img_tensor)\n",
    "\n",
    "    pred_boxes_all, pred_scores_all = [], []\n",
    "    strides = [8, 16, 32]\n",
    "\n",
    "    for i, (bbox_raw, cls_raw) in enumerate(preds):\n",
    "        stride = strides[i]\n",
    "        B, _, H, W = cls_raw.shape\n",
    "        cls_conf = cls_raw.sigmoid().squeeze(0).flatten()\n",
    "        keep = cls_conf > score_thresh\n",
    "        if keep.sum() == 0: continue\n",
    "\n",
    "        bbox_raw = bbox_raw.view(1, 4, reg_max + 1, H, W)\n",
    "        prob = F.softmax(bbox_raw, dim=2)\n",
    "        proj = torch.arange(reg_max + 1, device=prob.device, dtype=torch.float32)\n",
    "        dist = (prob * proj[None, None, :, None, None]).sum(dim=2)  # [1, 4, H, W]\n",
    "        dist = dist.squeeze(0).permute(1, 2, 0).reshape(-1, 4)\n",
    "\n",
    "        grid_y, grid_x = torch.meshgrid(torch.arange(H), torch.arange(W), indexing='ij')\n",
    "        grid = torch.stack([grid_x, grid_y], dim=-1).reshape(-1, 2).float().to(dist.device)\n",
    "\n",
    "        x_center = (grid[:, 0] + 0.5) * stride\n",
    "        y_center = (grid[:, 1] + 0.5) * stride\n",
    "        x1 = x_center - dist[:, 0] * stride\n",
    "        y1 = y_center - dist[:, 1] * stride\n",
    "        x2 = x_center + dist[:, 2] * stride\n",
    "        y2 = y_center + dist[:, 3] * stride\n",
    "\n",
    "        xc = (x1 + x2) / 2\n",
    "        yc = (y1 + y2) / 2\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1\n",
    "\n",
    "        boxes = torch.stack([xc, yc, w, h], dim=1)\n",
    "        boxes = boxes[keep]\n",
    "        scores = cls_conf[keep]\n",
    "\n",
    "        pred_boxes_all.append(boxes)\n",
    "        pred_scores_all.append(scores)\n",
    "\n",
    "    if not pred_boxes_all:\n",
    "        return []\n",
    "\n",
    "    final_boxes = torch.cat(pred_boxes_all, dim=0)\n",
    "    final_boxes /= input_size  # Normalize\n",
    "    final_boxes = final_boxes.clamp(0, 1)\n",
    "\n",
    "    labels = []\n",
    "    for i in range(final_boxes.shape[0]):\n",
    "        xc, yc, w, h = final_boxes[i]\n",
    "        labels.append(f\"0 {xc:.6f} {yc:.6f} {w:.6f} {h:.6f}\")\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def save_labels_for_folder(img_folder, model, out_dir, score_thresh=0.3):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    image_paths = glob(os.path.join(img_folder, \"*.*\"))\n",
    "\n",
    "    for img_path in image_paths:\n",
    "        labels = predict_yolo_labels(img_path, model, score_thresh=score_thresh)\n",
    "\n",
    "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        out_path = os.path.join(out_dir, base + \".txt\")\n",
    "\n",
    "        with open(out_path, \"w\") as f:\n",
    "            for line in labels:\n",
    "                f.write(line + \"\\n\")\n",
    "\n",
    "        print(f\"Saved {len(labels)} labels → {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-16T11:42:54.690976Z",
     "iopub.status.idle": "2025-06-16T11:42:54.691291Z",
     "shell.execute_reply": "2025-06-16T11:42:54.691132Z",
     "shell.execute_reply.started": "2025-06-16T11:42:54.691117Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = torch.load(\"/kaggle/working/yolov8_custom_vehicle_2.pt\", weights_only=False).eval().cuda()\n",
    "\n",
    "save_labels_for_folder(\n",
    "    img_folder=\"/kaggle/input/traffic-data/traffic_wala_dataset/valid/images\",    \n",
    "    model=model,\n",
    "    out_dir=\"C:/Users/DELL/Downloads/kaggle_saved_labels\",          \n",
    "    score_thresh=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "YOLO v8",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7647721,
     "isSourceIdPinned": false,
     "sourceId": 12142979,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7649896,
     "isSourceIdPinned": false,
     "sourceId": 12146168,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
